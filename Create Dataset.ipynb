{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48b2e565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/alyafey22/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534c4247a23a4efd95b14ee12e255e69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ashaar = load_dataset(\"arbml/ashaar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd53ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ashaar['train'] = ashaar['train'].select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9c7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_meters(sample):\n",
    "    meter = sample['poem meter']\n",
    "    remove_texts = ['مجزوء', 'مشطور', 'أحذ', 'مخلع', 'مربع', 'التفعيلة', 'منهوك', 'بحر', 'تفعيلة' , 'المقطوع']\n",
    "    if meter:\n",
    "        for removed in remove_texts:\n",
    "            sample['poem meter'] = sample['poem meter'].replace(removed, \"\").strip()\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443cd042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alyafey22/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-bee65d4186575ff7.arrow\n"
     ]
    }
   ],
   "source": [
    "ashaar['train'] = ashaar['train'].map(join_meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0f8a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = ['الكامل', 'المتدارك', 'الوافر', 'الرمل', 'السريع', 'الهزج', 'المقتضب', 'الطويل', 'المتقارب', 'المتدارك', 'المضارع', 'المديد', 'البسيط', 'الرجز', 'المنسرح', 'المجتث', 'الخفيف']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f26d776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/alyafey22/.cache/huggingface/datasets/arbml___parquet/MagedSaeed--ashaar-719bb58a76ea0092/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-cf1ee173cd1b4b0f.arrow\n"
     ]
    }
   ],
   "source": [
    "ashaar = ashaar.filter(lambda example: example[\"poem meter\"] in meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06f2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'المديد': '<|meter_0|>', 'السريع': '<|meter_1|>', 'الرجز': '<|meter_2|>', 'الكامل': '<|meter_3|>', 'المتقارب': '<|meter_4|>', 'المجتث': '<|meter_5|>', 'المضارع': '<|meter_6|>', 'الخفيف': '<|meter_7|>', 'الرمل': '<|meter_8|>', 'البسيط': '<|meter_9|>', 'الهزج': '<|meter_10|>', 'الطويل': '<|meter_11|>', 'المنسرح': '<|meter_12|>', 'المقتضب': '<|meter_13|>', 'المتدارك': '<|meter_14|>', 'الوافر': '<|meter_15|>'}\n"
     ]
    }
   ],
   "source": [
    "meters= {}\n",
    "unique_meters = set(x for x in ashaar['train']['poem meter'] if x is not None)\n",
    "meter_tokens = {meter:f'<|meter_{i}|>' for i, meter in enumerate(unique_meters)}\n",
    "print(meter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e627053",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_POEM_TOKEN = '<|psep|>'\n",
    "ED_POEM_TOKEN = '</|psep|>'\n",
    "VERSE_TOKEN = '<|vsep|>'\n",
    "ST_BAYT_TOKEN= '<|bsep|>'\n",
    "ED_BAYT_TOKEN= '</|bsep|>'\n",
    "PAD_TOKEN = '<|pad|>'\n",
    "EOT_TOKEN = '<|endoftext|>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "910c9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_verse(sample):\n",
    "    chars = 'ابتثجحخدذرزسشصضطظعغفقكلمنهويىئءأؤة ى'\n",
    "    diacs = 'ْ~ًٌٍَُِّ'\n",
    "    map_chars = {'ک':'ك', 'ﺑ':'ب', 'ٹ':'ث', 'ی':'ى'}\n",
    "    out = ''\n",
    "    for char in sample:\n",
    "        if char in chars+diacs:\n",
    "            out += char\n",
    "        elif char in map_chars:\n",
    "            out += map_chars[char]\n",
    "    return out.strip()\n",
    "\n",
    "def join_verses(sample):\n",
    "\n",
    "    verses = sample['poem verses']\n",
    "    meter = sample['poem meter']\n",
    "  \n",
    "    poem = ''.join([ f'{ST_BAYT_TOKEN} '+process_verse(verses[i]) +f' {VERSE_TOKEN} '+ process_verse(verses[i+1])+ f' {ED_BAYT_TOKEN} ' for i in range(0, len(verses)-2, 2)])\n",
    "    \n",
    "    if meter:\n",
    "        meter_token = meter_tokens[meter]\n",
    "    else:\n",
    "        meter_token = ''\n",
    "    text = f\"{meter_tokens[meter]} {ST_POEM_TOKEN} {poem.strip()} {ED_POEM_TOKEN}\"\n",
    "\n",
    "    return {'text':text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d353c023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/144188 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ashaar['train'] = ashaar['train'].map(join_verses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d7e9b799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem title': 'أصبح الملك للذي فطر الخلق',\n",
       " 'poem meter': 'الخفيف',\n",
       " 'poem verses': ['أَصبَحَ المُلك لِلَّذي فَطر الخَل',\n",
       "  'قَ بِتَقديرٍ للعَزيز العَليمِ',\n",
       "  'غافر الذَنب للمسيءِ بِعَفوٍ',\n",
       "  'قابل التَوب ذي العَطاء العَميمِ',\n",
       "  'مُرسل المُصطَفى البَشير إِلَينا',\n",
       "  'رَحمة مِنهُ بِالكَلام القَديمِ',\n",
       "  'رَبَنا رَبّنا إِلَيكَ أَنينا',\n",
       "  'فَأَجرنا مِن حَر نار الجَحيمِ',\n",
       "  'وَاكفِنا شَرّ ما نَخاف بِلُطفٍ',\n",
       "  'يا عَظيماً يَرجى لِكُل عَظيمِ',\n",
       "  'وَتَقبل أَعمالَنا وَاعفُ عَنا',\n",
       "  'وَأَنلنا دُخول دار النَعيمِ',\n",
       "  'بِنَبي بَعثَتهُ فَهَدانا',\n",
       "  'لِصِراط مِن الهُدى مُستَقيمِ',\n",
       "  'وَبِمَن نَحنُ في حِماهُ مَدى الدَهر',\n",
       "  'أَخيهِ يَحيى الحصور الكَريمِ',\n",
       "  'أَدرك أَدرك قَوماً أَتوا بافتقار',\n",
       "  'وَاِنكِسار وَمَدمَع مَسجومِ',\n",
       "  'شَهدت أَرواحَهُم أَنكَ اللَهُ',\n",
       "  'وَجاءوا بِكُل قَلبٍ سَليم'],\n",
       " 'poem theme': 'قصيدة دينية',\n",
       " 'poem url': 'https://www.aldiwan.net/poem16182.html',\n",
       " 'poet name': 'الامير منجك باشا',\n",
       " 'poet description': 'منجك بن محمد بن منجك بن ابي بكر بن عبد القادر بن ابراهيم بن منجك اليوسفي الكبير\\nاكبر شعراء عصره من اهل دمشق من بيت امارة و رياسة\\nانفق في صباه ما ورثه عن ابوه و انزوى ثم رحل الى الديار التركية و مدح السلطان ابراهيم و لم يظفر بطائل\\nفعاد الى دمشق و عاش فيها في ستر و جاه الى ان توفي بها.',\n",
       " 'poet url': 'https://www.aldiwan.net/cat-poet-alamir-mnczyk-pasha',\n",
       " 'poet era': 'العصر العثماني',\n",
       " 'poet location': None,\n",
       " 'poem description': None,\n",
       " 'poem language type': None,\n",
       " 'text': '<|meter_7|> <|psep|> <|bsep|> أَصبَحَ المُلك لِلَّذي فَطر الخَل <|vsep|> قَ بِتَقديرٍ للعَزيز العَليمِ </|bsep|> <|bsep|> غافر الذَنب للمسيءِ بِعَفوٍ <|vsep|> قابل التَوب ذي العَطاء العَميمِ </|bsep|> <|bsep|> مُرسل المُصطَفى البَشير ِلَينا <|vsep|> رَحمة مِنهُ بِالكَلام القَديمِ </|bsep|> <|bsep|> رَبَنا رَبّنا ِلَيكَ أَنينا <|vsep|> فَأَجرنا مِن حَر نار الجَحيمِ </|bsep|> <|bsep|> وَاكفِنا شَرّ ما نَخاف بِلُطفٍ <|vsep|> يا عَظيماً يَرجى لِكُل عَظيمِ </|bsep|> <|bsep|> وَتَقبل أَعمالَنا وَاعفُ عَنا <|vsep|> وَأَنلنا دُخول دار النَعيمِ </|bsep|> <|bsep|> بِنَبي بَعثَتهُ فَهَدانا <|vsep|> لِصِراط مِن الهُدى مُستَقيمِ </|bsep|> <|bsep|> وَبِمَن نَحنُ في حِماهُ مَدى الدَهر <|vsep|> أَخيهِ يَحيى الحصور الكَريمِ </|bsep|> <|bsep|> أَدرك أَدرك قَوماً أَتوا بافتقار <|vsep|> وَاِنكِسار وَمَدمَع مَسجومِ </|bsep|> </|psep|>'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ashaar['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "86edc15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "def batch_iterator():\n",
    "    for i in range(0, len(ashaar['train']), batch_size):\n",
    "        yield ashaar['train'][i : i + batch_size][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cac1b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = \"\"\n",
    "for text in ashaar['train'][\"text\"]:\n",
    "    full_dataset += text\n",
    "char_cnt =len(set(list(full_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d69351a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "print(char_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3508d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1', '7', 'ة', 'س', 'ح', '<', 'p', 'ً', 'ف', 's', 'و', 'ُ', 'ك', 'ٍ', 'r', '5', 'ص', 'ط', 'ق', '9', 'د', 'ي', 'ض', '_', 'خ', 'ء', 'ث', '0', '~', 'ى', 'e', 'm', ' ', 'َ', 'ٌ', 'ر', 'v', 'ج', 'م', 'ا', 'ن', 'ه', 'ؤ', 'ظ', 'b', '2', '/', 'ت', '8', 'ّ', 't', 'ل', '3', 'ِ', 'ب', 'ذ', 'ز', 'أ', 'ع', 'غ', 'ش', 'ئ', 'ْ', '4', '6', '>', '|'}\n"
     ]
    }
   ],
   "source": [
    "print(set(list(full_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8044ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|meter_0|>', '<|meter_1|>', '<|meter_2|>', '<|meter_3|>', '<|meter_4|>', '<|meter_5|>', '<|meter_6|>', '<|meter_7|>', '<|meter_8|>', '<|meter_9|>', '<|meter_10|>', '<|meter_11|>', '<|meter_12|>', '<|meter_13|>', '<|meter_14|>', '<|meter_15|>']\n"
     ]
    }
   ],
   "source": [
    "print(list(meter_tokens.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a806b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|res_0|>', '<|res_1|>', '<|res_2|>', '<|res_3|>', '<|res_4|>', '<|res_5|>', '<|res_6|>', '<|res_7|>', '<|res_8|>', '<|res_9|>']\n"
     ]
    }
   ],
   "source": [
    "extra_tokens = [f'<|res_{i}|>' for i in range(10)]\n",
    "print(extra_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cb76dfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers, Tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "#This pre-tokenizer takes care of replacing all bytes of the given string with a corresponding representation, as well as splitting into words.\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.UnicodeScripts() #add space whether to add space at the first word\n",
    "special_tokens = [EOT_TOKEN, ST_POEM_TOKEN, ED_POEM_TOKEN, VERSE_TOKEN, ST_BAYT_TOKEN, ED_BAYT_TOKEN, PAD_TOKEN]+list(meter_tokens.values())+extra_tokens\n",
    "trainer = trainers.BpeTrainer(vocab_size=char_cnt+len(special_tokens), special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n",
    "tokenizer.pad_token = PAD_TOKEN\n",
    "print(tokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "147bebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, AutoTokenizer\n",
    "gpt_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d7c8050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<|endoftext|>', '<|endoftext|>'), ('<|psep|>', '<|psep|>'), ('</|psep|>', '</|psep|>'), ('<|vsep|>', '<|vsep|>'), ('<|bsep|>', '<|bsep|>'), ('</|bsep|>', '</|bsep|>'), ('<|pad|>', '<|pad|>'), ('<|meter_0|>', '<|meter_0|>'), ('<|meter_1|>', '<|meter_1|>'), ('<|meter_2|>', '<|meter_2|>'), ('<|meter_3|>', '<|meter_3|>'), ('<|meter_4|>', '<|meter_4|>'), ('<|meter_5|>', '<|meter_5|>'), ('<|meter_6|>', '<|meter_6|>'), ('<|meter_7|>', '<|meter_7|>'), ('<|meter_8|>', '<|meter_8|>'), ('<|meter_9|>', '<|meter_9|>'), ('<|meter_10|>', '<|meter_10|>'), ('<|meter_11|>', '<|meter_11|>'), ('<|meter_12|>', '<|meter_12|>'), ('<|meter_13|>', '<|meter_13|>'), ('<|meter_14|>', '<|meter_14|>'), ('<|meter_15|>', '<|meter_15|>'), ('<|res_0|>', '<|res_0|>'), ('<|res_1|>', '<|res_1|>'), ('<|res_2|>', '<|res_2|>'), ('<|res_3|>', '<|res_3|>'), ('<|res_4|>', '<|res_4|>'), ('<|res_5|>', '<|res_5|>'), ('<|res_6|>', '<|res_6|>'), ('<|res_7|>', '<|res_7|>'), ('<|res_8|>', '<|res_8|>'), ('<|res_9|>', '<|res_9|>')]\n"
     ]
    }
   ],
   "source": [
    "print([(gpt_tokenizer.tokenize(sp)[0], sp) for sp in special_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2f1e7078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gpt_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2cae1c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f869afab07ee448381a5f5881ed1879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b162eb6ff81443aa8fc94cd7f0dfc4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/73 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02518e5a9411422ebe62e12ff34f2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/73 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ae7410159e454ab1c8008b8024130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0e69e1055245fe9d43609bfecd66a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/4.25k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ashaar.push_to_hub('ashaar_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4624d3c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Zaid/ashaar_tokenizer/commit/5f50cb4918e5507094340d52428c375a244766aa', commit_message='Upload tokenizer', commit_description='', oid='5f50cb4918e5507094340d52428c375a244766aa', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.push_to_hub('ashaar_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b07dfadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast, AutoTokenizer\n",
    "\n",
    "gpt_tokenizer = GPT2TokenizerFast.from_pretrained('Zaid/ashaar_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb5eb24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ظ': 79, 'م': 86, 'ي': 91, 'ض': 77, '<|res_2|>': 25, 't': 54, 'ْ': 99, '_': 47, '<|endoftext|>': 0, '1': 36, '6': 41, '>': 46, 'ر': 72, '<|meter_9|>': 16, '<|meter_12|>': 19, '~': 57, 'ث': 66, 'ٌ': 93, '<|meter_0|>': 7, '8': 43, 'p': 51, '</|bsep|>': 5, '<|meter_5|>': 12, 'ن': 87, '<|psep|>': 1, '<|vsep|>': 3, '2': 37, 'ى': 90, 'ب': 63, '<|meter_10|>': 17, 'خ': 69, 'ف': 82, '9': 44, '<|res_1|>': 24, '<|meter_4|>': 11, '<|res_7|>': 30, '|': 56, 'ٍ': 94, 'ّ': 98, '<|res_5|>': 28, 'غ': 81, '7': 42, '<|meter_11|>': 18, 'e': 49, 'ؤ': 60, '<|meter_13|>': 20, '<|meter_6|>': 13, '4': 39, '<|meter_15|>': 22, '<|res_3|>': 26, '<|pad|>': 6, '<|meter_14|>': 21, '<|meter_8|>': 15, 'ئ': 61, 'ة': 64, 'ح': 68, 'ص': 76, 'س': 74, '<': 45, 'm': 50, 'ش': 75, '<|res_8|>': 31, 'ل': 85, 'ً': 92, '<|bsep|>': 4, '<|res_6|>': 29, 's': 53, 'v': 55, 'r': 52, '<|res_4|>': 27, 'ق': 83, 'ع': 80, 'َ': 95, 'ُ': 96, '<|meter_1|>': 8, 'ِ': 97, 'د': 70, 'ط': 78, 'ء': 58, 'ك': 84, '<|meter_2|>': 9, 'ذ': 71, '/': 34, '<|meter_7|>': 14, 'ج': 67, 'ا': 62, '<|res_9|>': 32, '0': 35, 'و': 89, '<|meter_3|>': 10, ' ': 33, 'أ': 59, 'ه': 88, '3': 38, 'ت': 65, '5': 40, 'ز': 73, '</|psep|>': 2, '<|res_0|>': 23, 'b': 48}\n"
     ]
    }
   ],
   "source": [
    "print(gpt_tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9301326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n",
      "p\n",
      "e\n",
      "m\n",
      "s\n",
      "v\n",
      "r\n",
      "/\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "chars = 'ابتثجحخدذرزسشصضطظعغفقكلمنهويىئءأؤة ى'\n",
    "diacs = 'ْ~ًٌٍَُِّ'\n",
    "en_chars = 'abcdefghijklmnopqrstuvwxyz/'\n",
    "for vocab in gpt_tokenizer.get_vocab():\n",
    "    if '<' in vocab and '>' in vocab:\n",
    "#         print(vocab)\n",
    "        continue\n",
    "    elif vocab in chars +diacs:\n",
    "        continue\n",
    "    elif vocab in en_chars:\n",
    "        print(vocab)\n",
    "    else:\n",
    "#         print(vocab)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe071988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ظ': 79, 'م': 86, 'ي': 91, 'ض': 77, '<|res_2|>': 25, 't': 54, 'ْ': 99, '_': 47, '<|endoftext|>': 0, '1': 36, '6': 41, '>': 46, 'ر': 72, '<|meter_9|>': 16, '<|meter_12|>': 19, '~': 57, 'ث': 66, 'ٌ': 93, '<|meter_0|>': 7, '8': 43, 'p': 51, '</|bsep|>': 5, '<|meter_5|>': 12, 'ن': 87, '<|psep|>': 1, '<|vsep|>': 3, '2': 37, 'ى': 90, 'ب': 63, '<|meter_10|>': 17, 'خ': 69, 'ف': 82, '9': 44, '<|res_1|>': 24, '<|meter_4|>': 11, '<|res_7|>': 30, '|': 56, 'ٍ': 94, 'ّ': 98, '<|res_5|>': 28, 'غ': 81, '7': 42, '<|meter_11|>': 18, 'e': 49, 'ؤ': 60, '<|meter_13|>': 20, '<|meter_6|>': 13, '4': 39, '<|meter_15|>': 22, '<|res_3|>': 26, '<|pad|>': 6, '<|meter_14|>': 21, '<|meter_8|>': 15, 'ئ': 61, 'ة': 64, 'ح': 68, 'ص': 76, 'س': 74, '<': 45, 'm': 50, 'ش': 75, '<|res_8|>': 31, 'ل': 85, 'ً': 92, '<|bsep|>': 4, '<|res_6|>': 29, 's': 53, 'v': 55, 'r': 52, '<|res_4|>': 27, 'ق': 83, 'ع': 80, 'َ': 95, 'ُ': 96, '<|meter_1|>': 8, 'ِ': 97, 'د': 70, 'ط': 78, 'ء': 58, 'ك': 84, '<|meter_2|>': 9, 'ذ': 71, '/': 34, '<|meter_7|>': 14, 'ج': 67, 'ا': 62, '<|res_9|>': 32, '0': 35, 'و': 89, '<|meter_3|>': 10, ' ': 33, 'أ': 59, 'ه': 88, '3': 38, 'ت': 65, '5': 40, 'ز': 73, '</|psep|>': 2, '<|res_0|>': 23, 'b': 48}\n"
     ]
    }
   ],
   "source": [
    "print(gpt_tokenizer.get_vocab())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
